
# coding: utf-8

# In[ ]:


import tensorflow as tf
import numpy as np
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import time
from datetime import timedelta
import math
import _pickle as cPickle
import os


# In[ ]:


def load_data(folder):
    x = []
    y = []
    for path in os.listdir(folder):
        input = cPickle.load(open(os.path.join(folder, path), 'rb'))
        x.append(input['x'])
        y.append(input['y'])
    return x, y


# In[ ]:


x, y = load_data('./../Pickles')


# In[ ]:


x, y = shuffle(x, y, random_state=137)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=137)


# In[ ]:


# Each opcode sequence in one-hot form is of dimensions N x 256
op_rows = 100000
op_cols = 1

# Opcode sequences are stored in one-dimensional arrays of this length
op_size_flat = op_rows * op_cols

# Tuple with height and width of opcode sequences used to reshape arrays
op_shape = (op_rows, op_cols)

num_classes = 2

num_channels = 1


# In[ ]:


def new_weights(shape):
	return tf.Variable(tf.truncated_normal(shape, stddev=0.05))

def new_biases(length):
	return tf.Variable(tf.constant(0.05, shape=[length]))


# In[ ]:


x_ints = tf.placeholder(tf.int32, shape=[1, op_size_flat], name='x')

x_embeddings = tf.placeholder(tf.float32, shape = [1, op_size_flat, 8, 1], name='x_embed')

# Placeholder for true labels
y_true = tf.placeholder(tf.float32, shape=[1, num_classes], name='y_true')
# Placeholder variable for class number
y_true_cls = tf.argmax(y_true, axis=1, name='y_true_cls')


# In[ ]:


# tf.reset_default_graph()


# In[ ]:


embeddings = tf.Variable(
    tf.random_uniform(
    [218,8],
    minval=-1,
    maxval=1,
    dtype=tf.float32,
    seed=None),
    name='Variable'
)


# In[ ]:


# Restore Embedding Layer : get "embeddings"
restore_sess = tf.Session()


# In[ ]:


restorer = tf.train.Saver(var_list=[embeddings])


# In[ ]:


# restore_sess.run(embeddings).shape


# In[ ]:


# restore_sess.run(embeddings).shape


# In[ ]:


restorer.restore(restore_sess, './Saved Model/model.cpkt')


# In[ ]:


# restore_sess.run(embeddings)


# In[ ]:


# embed = tf.nn.embedding_lookup(embeddings, x_ints)
# embed = tf.reshape(embed, shape=[1, 100000, 8, 1])
# print(embed.shape)


# In[ ]:


# x_embeddings = embed
# print(x_embeddings)


# In[ ]:


def new_fc_layer(input,          # The previous layer
                 num_inputs,     # Number of inputs from previous layer
                 num_outputs,    # Number of outputs
                 use_relu=True, name="fc"): # Use Rectified Linear Unit (ReLU)?

    with tf.name_scope(name):
        # Create new weights and biases.
        weights = new_weights(shape=[num_inputs, num_outputs])
    #     weights = tf.expand_dims(weights,0)
        biases = new_biases(length=num_outputs)

        # Calculate the layer as the matrix multiplication of
        # the input and weights, and then add the bias-values.
        layer = tf.matmul(input, weights) + biases

        # Use ReLU?
        if use_relu:
            layer = tf.nn.relu(layer)
        tf.summary.histogram(name + "/weights", weights)
        tf.summary.histogram(name + "/biases", biases)
        tf.summary.histogram(name+ "/activations", layer)
        return layer


# In[ ]:


def new_conv_layer(input,              	# The input layer
                   num_input_channels, 	# Number of channels in input layer
                   filter_size,        	# Width and height of each filter
                   num_filters,        	# Number of filters
                   padding='SAME',
                   use_pooling=False, name="conv"):  # Max Pooling not done
    
    with tf.name_scope(name):
        # Shape of the filter-weights for the convolution.
        shape = [filter_size, filter_size, num_input_channels, num_filters]

        # Create new weights aka. filters with the given shape.
        weights = new_weights(shape=shape)

        # Create new biases, one for each filter.
        biases = new_biases(length=num_filters)

        # Create the TensorFlow operation for convolution.
        layer = tf.nn.conv2d(input=input,
                             filter=weights,
                             strides=[1, 1, 1, 1],
                             padding=padding)

        # Add the biases to the results of the convolution.
        layer += biases

        # Use pooling to down-sample the image resolution?
        if use_pooling:
            # This is 2x2 max-pooling, which means that we
            # consider 2x2 windows and select the largest value
            # in each window. Then we move 2 pixels to the next window.
            layer = tf.nn.max_pool(value=layer,
                                   ksize=[1, 2, 2, 1],
                                   strides=[1, 2, 2, 1],
                                   padding=padding)

        # Rectified Linear Unit (ReLU).
        layer = tf.nn.relu(layer)
        tf.summary.histogram(name + "/weights", weights)
        tf.summary.histogram(name + "/biases", biases)
        tf.summary.histogram(name+ "/activations", layer)
        return layer, weights


# In[ ]:


def new_conv_transpose_layer(input,              # The input layer
                             num_input_channels, # Number of channels in input layer
                             filter_size,        # Width and height of each filter
                             num_filters,
                             weights,
                             output_shape,
                             name="conv_trans"):       # Max Pooling not done
    
    with tf.name_scope(name):
        # Shape of the filter-weights for transpose_convolution.
        shape = [filter_size, filter_size, num_filters, num_input_channels]

        # Create new weights aka. filters with the given shape.
        weights = new_weights(shape=shape)

        # Create new biases, one for each filter.
        biases = new_biases(length=num_filters)

#         output_shape = [0,0,0,0]
#         output_shape[0] = input.get_shape().as_list()[0]
#         output_shape[1] = 100000
#         output_shape[2] = 8
#         output_shape[3] = 1
#         batch_size = input.get_shape().as_list()[0]
        
        # Create the TensorFlow operation for convolution.
        layer = tf.nn.conv2d_transpose(value=input,
                                       filter=weights_conv,
                                       output_shape=output_shape,
#                                        output_shape= tf.stack(output_shape),
                                       strides=[1, 2, 2, 1],
                                       padding='SAME')

        # Add the biases to the results of the convolution.
        layer += biases

        # Use pooling to down-sample the image resolution?
#         if use_pooling:
#             # This is 2x2 max-pooling, which means that we
#             # consider 2x2 windows and select the largest value
#             # in each window. Then we move 2 pixels to the next window.
#             layer = tf.nn.max_pool(value=layer,
#                                    ksize=[1, 2, 2, 1],
#                                    strides=[1, 2, 2, 1],
#                                    padding='SAME')

        # Rectified Linear Unit (ReLU).
        layer = tf.nn.relu(layer)
        tf.summary.histogram(name + "/weights", weights)
        tf.summary.histogram(name + "/biases", biases)
        tf.summary.histogram(name+ "/activations", layer)
        return layer, weights


# In[ ]:


# print(x_embeddings.shape)
# print(embeddings.shape)
# embed = tf.nn.embedding_lookup(embeddings, x_ints)
# print(embed.shape)
# x_embeddings = embed


# In[ ]:


# Perform Lookup
embed = tf.nn.embedding_lookup(embeddings, x_ints)
embed = tf.reshape(embed, shape=[1, 100000, 8, 1])


# In[ ]:


layer_conv, weights_conv =     new_conv_layer(input=x_embeddings,
                   num_input_channels=1,
                   filter_size=4,
                   num_filters=16,
                   use_pooling=True, name="conv1")
print(layer_conv)


# In[ ]:


layer_conv_trans, weights_deconv_trans =     new_conv_transpose_layer(input=layer_conv,
                            num_input_channels=16,
                            num_filters=1,
                            weights=weights_conv,
                            output_shape=embed.shape,
                            filter_size=4)
print(layer_conv_trans)
print(embed)


# In[ ]:


print(layer_conv)


# In[ ]:


# list = [1,1,1,1]
# tf.convert_to_tensor(list)


# In[ ]:


cost = tf.reduce_sum(tf.square(layer_conv_trans - x_embeddings))


# In[ ]:


learning_rate = 1e-4
optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)


# In[ ]:


session = tf.Session()


# In[ ]:


session.run(tf.global_variables_initializer())


# In[ ]:


def optimize(num_epochs, batch_size):
    counter=0
    for epoch in range(num_epochs):
        for step in range(int(len(x_train)/batch_size)):
            batch_x = x_train[step*batch_size:(step+1)*batch_size]
            feed_dict_train = {x_ints: batch_x}
            x_embd = session.run(embed, feed_dict=feed_dict_train)
            feed_dict_train = {x_embeddings: x_embd}
            session.run(optimizer, feed_dict=feed_dict_train)
            print(counter)
            counter = counter+1
            
#             msg = "Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}"

#             # Print it.
#             print(msg.format(counter + 1, acc))
#             counter = counter + 1


# In[ ]:


train_batch_size=1
optimize(num_epochs=5, batch_size=train_batch_size)


# In[ ]:


ae_saver = tf.train.Saver()
ae_saver.save(session, './ae_model.cpkt')


# In[ ]:


# ae_recoverer = tf.train.Saver()
# ae_recoverer.restore(session, './ae_model.cpkt')


# In[ ]:


x_reduced = tf.placeholder(tf.float32, shape=[1, 50000, 4, 16], name='x_red')


# In[ ]:


print(x_reduced)


# In[ ]:


final_conv, final_weights =     new_conv_layer(input=x_reduced,
                   num_input_channels=16,
                   filter_size=4,
                   num_filters=64,
                   padding='VALID',
                   use_pooling=False, name="final_conv")
print(final_conv)


# In[ ]:


max_layer = tf.reduce_max(
    final_conv,
    axis=1,
#     keepdims=False,
    name=None,
    reduction_indices=None,
    keep_dims=False
)
max_layer = tf.reshape(max_layer, shape=[-1,64])


# In[ ]:


print(max_layer)


# In[ ]:


layer_fc = new_fc_layer(input=max_layer,
                         num_inputs=64,
                         num_outputs=2,
                         use_relu=False, name="fc1")
print(layer_fc)


# In[ ]:


y_pred = tf.nn.softmax(layer_fc)


# In[ ]:


y_pred_cls = tf.argmax(y_pred, axis=1)


# In[ ]:


cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = layer_fc, labels=y_true)
xent_cost = tf.reduce_mean(cross_entropy)


# In[ ]:


xent_optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(xent_cost)


# In[ ]:


with tf.name_scope("accuracy"):
    correct_prediction = tf.equal(y_pred_cls, y_true_cls)
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    tf.summary.scalar("accuracy", accuracy)


# In[ ]:


writer = tf.summary.FileWriter("ae/1", graph=tf.get_default_graph())
merged_summary = tf.summary.merge_all()


# In[ ]:


sess = tf.Session()


# In[ ]:


sess.run(tf.global_variables_initializer())


# In[ ]:


def optimize(num_epochs, batch_size):
    counter=0
    for epoch in range(num_epochs):
        for step in range(int(len(x_train)/batch_size)):
            batch_x = x_train[step*batch_size:(step+1)*batch_size]
            feed_dict_train = {x_ints: batch_x}
            x_embd = sess.run(embed, feed_dict=feed_dict_train)
            feed_dict_train = {x_embeddings: x_embd}
            x_rdcd = sess.run(layer_conv, feed_dict=feed_dict_train)
            batch_y = y_train[step*batch_size:(step+1)*batch_size]
            b = np.zeros((batch_size,2))
            b[np.arange(batch_size),np.asarray(batch_y)] = 1
            batch_y = np.float32(b)
#             assert(batch_x.shape[0]==batch_y.shape[0])
            feed_dict_train = {x_reduced: x_rdcd,
                               y_true: batch_y}
            sess.run(xent_optimizer, feed_dict=feed_dict_train)
            
            acc = sess.run(accuracy, feed_dict=feed_dict_train)
#             summary = sess.run(merged_summary, feed_dict=feed_dict_train)
#             writer.add_summary(summary, counter)
            
            msg = "Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}"
            print(batch_y)
            # Print it.
            print(msg.format(counter + 1, acc))
            counter = counter + 1


# In[ ]:


train_batch_size = 1
optimize(num_epochs=5, batch_size=train_batch_size)


# In[ ]:


saver = tf.train.Saver()
saver.save(sess, './classifier.cpkt')


# In[ ]:


# saver = tf.train.Saver()
# saver.restore(sess, './classifier.cpkt')


# In[ ]:


b = np.zeros((584,2))
b[np.arange(584),np.asarray(y_test)] = 1
y_test = np.float32(b)
feed_dict_test = {x_ints: x_test,
                y_true: y_test}


# In[ ]:


batch_size = 1
avg_accuracy = []
for i in range(int(584 / batch_size)):
    batch_x = x_test[i * batch_size:(i + 1) * batch_size]
    feed_dict_test = {x_ints: batch_x}
    x_embd = sess.run(embed, feed_dict=feed_dict_test)
    feed_dict_test = {x_embeddings: x_embd}
    x_rdcd = sess.run(layer_conv, feed_dict=feed_dict_test)
    batch_y = y_test[i * batch_size:(i + 1) * batch_size]
    feed_dict_test = {x_reduced: x_rdcd, y_true: batch_y}
    acc = sess.run(accuracy, feed_dict=feed_dict_test)
    avg_accuracy.append(acc)
    print(batch_y)
    print("Batch: %d -- Accuracy: %g" %(i, acc))


# In[ ]:


print("Average Accuracy: %g", np.mean(avg_accuracy))


# In[ ]:


tp, tn, fp, fn = 0, 0, 0, 0


# In[ ]:


batch_size = 1
for i in range(int(584/batch_size)):
    batch_x = x_test[i * batch_size:(i+1) * batch_size]
    feed_dict_test = {x_ints: batch_x}
    x_embd = sess.run(embed, feed_dict=feed_dict_test)
    feed_dict_test = {x_embeddings: x_embd}
    x_rdcd = sess.run(layer_conv, feed_dict=feed_dict_test)
    batch_y = y_test[i * batch_size:(i+1) * batch_size]
    feed_dict_test = {x_reduced: x_rdcd, y_true: batch_y}
    [y_predict] = sess.run(y_pred_cls, feed_dict=feed_dict_test)
    if y_predict == 1 and y_test[i][0] == 0:
        tn += 1
    elif y_predict == 1 and y_test[i][0] == 1:
        fp += 1
    elif y_predict == 0 and y_test[i][0] == 0:
        fn += 1
    else:
        tp += 1


# In[ ]:


print(tp, fp, fn, tn)


# In[ ]:


precision = tp / (tp + fp)
recall = tp / (tp + fn)
print("Precison: " + str(precision))
print("Recall: " + str(recall))


# In[ ]:


from tensorflow.python.tools import inspect_checkpoint as ckpt


# In[ ]:


ckpt.print_tensors_in_checkpoint_file('./ae_model.cpkt', tensor_name='', all_tensors=False, all_tensor_names=True)


# In[ ]:


ckpt.print_tensors_in_checkpoint_file('./classifier.cpkt', tensor_name='', all_tensors=False, all_tensor_names=True)


# In[ ]:


from tensorflow.python import pywrap_tensorflow


# In[ ]:


reader = pywrap_tensorflow.NewCheckpointReader('Saved Model/model.cpkt')


# In[ ]:


reader.get_tensor('conv1/Variable_1')


# In[ ]:


reader = pywrap_tensorflow.NewCheckpointReader('./classifier.cpkt')


# In[ ]:


reader.get_tensor('conv1/Variable')


# In[ ]:


reader.get_tensor('conv_trans/Variable')


# In[ ]:


# restore_sess.close()
# sess.close()
# session.close()

