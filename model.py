
%matplotlib inline
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
from sklearn.metrics import confusion_matrix
import time
from datetime import timedelta
import math






# Embedding Layer
num_rows = ?? 
num_cols = 8

# Convolutional Layer
filter_size = 8 
num_filters = 64

# Max Pooling Layer


# Fully-connected Layer
fc_size = 16      





# Loading Data





# Convert Data into One-hot form







# Each opcode sequence in one-hot form is of dimensions N x 256
op_rows = ??
op_cols = 256

# Opcode sequences are stored in one-dimensional arrays of this length
op_size_flat = op_rows * op_cols

# Tuple with height and width of opcode sequences used to reshape arrays
op_shape = (op_rows, op_cols)

# Number of channels for the input: 1
num_channels = 1

# Number of classes
num_classes = 2






def new_weights(shape):
	return tf.Variable(tf.truncated_normal(shape, stddev=0.05))

def new_biases(length):
	return tf.Variable(tf.constant(0.05, shape=[length]))






def new_conv_layer(input,              	# The input layer
                   num_input_channels, 	# Number of channels in input layer
                   filter_size,        	# Width and height of each filter
                   num_filters,        	# Number of filters
                   use_pooling=False):  # Max Pooling not done

    # Shape of the filter-weights for the convolution.
    shape = [filter_size, filter_size, num_input_channels, num_filters]

    # Create new weights aka. filters with the given shape.
    weights = new_weights(shape=shape)

    # Create new biases, one for each filter.
    biases = new_biases(length=num_filters)

    # Create the TensorFlow operation for convolution.
    layer = tf.nn.conv2d(input=input,
                         filter=weights,
                         strides=[1, 1, 1, 1],
                         padding='SAME')

    # Add the biases to the results of the convolution.
    layer += biases

    # Use pooling to down-sample the image resolution?
    if use_pooling:
        # This is 2x2 max-pooling, which means that we
        # consider 2x2 windows and select the largest value
        # in each window. Then we move 2 pixels to the next window.
        layer = tf.nn.max_pool(value=layer,
                               ksize=[1, 2, 2, 1],
                               strides=[1, 2, 2, 1],
                               padding='SAME')

    # Rectified Linear Unit (ReLU).
    layer = tf.nn.relu(layer)

    return layer, weights






def flatten_layer(layer):
    # Get the shape of the input layer.
    layer_shape = layer.get_shape()

    num_features = layer_shape[1:4].num_elements()
    
    layer_flat = tf.reshape(layer, [-1, num_features])

    return layer_flat, num_features







def new_fc_layer(input,          # The previous layer
                 num_inputs,     # Number of inputs from previous layer
                 num_outputs,    # Number of outputs
                 use_relu=True): # Use Rectified Linear Unit (ReLU)?

    # Create new weights and biases.
    weights = new_weights(shape=[num_inputs, num_outputs])
    biases = new_biases(length=num_outputs)

    # Calculate the layer as the matrix multiplication of
    # the input and weights, and then add the bias-values.
    layer = tf.matmul(input, weights) + biases

    # Use ReLU?
    if use_relu:
        layer = tf.nn.relu(layer)

    return layer






